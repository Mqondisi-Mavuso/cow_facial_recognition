{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c32027f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9d6e40",
   "metadata": {},
   "outputs": [
    {
     "name": " stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in Thecowface-1 to yolov8: 100% [7540144 / 7540144] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to Thecowface-1 in yolov8:: 100%|████████████████████████| 282/282 [00:00<00:00, 603.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# This is for downloading the YOLOv8 cow face images and labels\n",
    "#!pip install roboflow\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"my_api_here\")                    #remeber to commit this to public repo\n",
    "project = rf.workspace(\"face-cow\").project(\"thecowface\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745cafdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c17df02c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1661027642.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [19]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install ultralytics==8.0.20\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pip install ultralytics==8.0.20\n",
    "# This is just to check the Python environment \n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "!yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afefb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f8b89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  Ultralytics settings reset to defaults. \n",
      "This is normal and may be due to a recent ultralytics package update, but may have overwritten previous settings. \n",
      "You may view and update settings directly in 'C:\\Users\\Fortune\\AppData\\Roaming\\Ultralytics\\settings.yaml'\n",
      "Ultralytics YOLOv8.0.20  Python-3.9.12 torch-1.13.1+cpu CPU\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.yaml, data=data.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, cache=False, device=, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=False, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, save_dir=runs\\segment\\train2\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to C:\\Users\\Fortune\\AppData\\Roaming\\Ultralytics\\Arial.ttf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1193dd7d0dd43b1b4d0d388abd05c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/755k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.Segment               [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.001), 76 bias\n",
      "train: Scanning D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\labScanning D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\labScanning D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\labScanning D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\lab\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\images\\00000193.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  No labels found in D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\labels.cache. See https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\labels.cache\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\labels.cache, can not start training. See https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8n-seg.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:207\u001b[0m, in \u001b[0;36mYOLO.train\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# update model and cfg after training\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, _ \u001b[38;5;241m=\u001b[39m attempt_load_one_weight(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mbest))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\trainer.py:183\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m         ddp_cleanup(command, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\trainer.py:248\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, rank, world_size)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_ddp(rank, world_size)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\trainer.py:234\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[1;34m(self, rank, world_size)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# dataloaders\u001b[39;00m\n\u001b[0;32m    233\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m world_size \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m}:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\v8\\detect\\train.py:43\u001b[0m, in \u001b[0;36mDetectionTrainer.get_dataloader\u001b[1;34m(self, dataset_path, batch_size, mode, rank)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_path, batch_size, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# TODO: manage splits differently\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# calculate stride - check if model is initialized\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(de_parallel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_dataloader(path\u001b[38;5;241m=\u001b[39mdataset_path,\n\u001b[0;32m     29\u001b[0m                              imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz,\n\u001b[0;32m     30\u001b[0m                              batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     31\u001b[0m                              stride\u001b[38;5;241m=\u001b[39mgs,\n\u001b[0;32m     32\u001b[0m                              hyp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs),\n\u001b[0;32m     33\u001b[0m                              augment\u001b[38;5;241m=\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m                              cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mcache,\n\u001b[0;32m     35\u001b[0m                              pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     36\u001b[0m                              rect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrect,\n\u001b[0;32m     37\u001b[0m                              rank\u001b[38;5;241m=\u001b[39mrank,\n\u001b[0;32m     38\u001b[0m                              workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers,\n\u001b[0;32m     39\u001b[0m                              close_mosaic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mclose_mosaic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     40\u001b[0m                              prefix\u001b[38;5;241m=\u001b[39mcolorstr(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     41\u001b[0m                              shuffle\u001b[38;5;241m=\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     42\u001b[0m                              seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mv5loader \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[1;32m---> 43\u001b[0m         \u001b[43mbuild_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\data\\build.py:64\u001b[0m, in \u001b[0;36mbuild_dataloader\u001b[1;34m(cfg, batch_size, img_path, stride, label_path, rank, mode)\u001b[0m\n\u001b[0;32m     62\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(rank):  \u001b[38;5;66;03m# init dataset *.cache only once if DDP\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mYOLODataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# augmentation\u001b[39;49;00m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: probably add a get_hyps_from_cfg function\u001b[39;49;00m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# rectangular batches\u001b[39;49;00m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_cls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_segments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_keypoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeypoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(batch_size, \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[0;32m     81\u001b[0m nd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()  \u001b[38;5;66;03m# number of CUDA devices\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\data\\dataset.py:45\u001b[0m, in \u001b[0;36mYOLODataset.__init__\u001b[1;34m(self, img_path, imgsz, label_path, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, use_segments, use_keypoints)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_keypoints \u001b[38;5;241m=\u001b[39m use_keypoints\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_segments \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_keypoints), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not use both segments and keypoints.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\data\\base.py:51\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[1;34m(self, img_path, imgsz, label_path, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;241m=\u001b[39m prefix\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_img_files(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path)\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_cls:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_labels(include_class\u001b[38;5;241m=\u001b[39m[])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\data\\dataset.py:116\u001b[0m, in \u001b[0;36mYOLODataset.get_labels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsgs\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    115\u001b[0m         LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsgs\u001b[39m\u001b[38;5;124m\"\u001b[39m]))  \u001b[38;5;66;03m# display warnings\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m nf \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mNo labels found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, can not start training. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHELP_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Read cache\u001b[39;00m\n\u001b[0;32m    119\u001b[0m [cache\u001b[38;5;241m.\u001b[39mpop(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsgs\u001b[39m\u001b[38;5;124m\"\u001b[39m)]  \u001b[38;5;66;03m# remove items\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in D:\\Fortune\\Education\\University\\111_Spring_Semester\\U_P_II\\cow_facial_recognition\\face_train_data\\train\\labels.cache, can not start training. See https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data"
     ]
    }
   ],
   "source": [
    "#training the model to segment the cow face using custom dataset \n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-seg.pt\")  # load a pretrained model (recommended for training)\n",
    "# Train the model\n",
    "model.train(data=\"data.yaml\", epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5684b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\fortune\\anaconda3\\lib\\site-packages (0.2.29)\n",
      "Collecting idna==2.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "label-studio 1.7.1 requires boto3~=1.16.28, but you have boto3 1.24.28 which is incompatible.\n",
      "label-studio 1.7.1 requires botocore~=1.19.28, but you have botocore 1.27.59 which is incompatible."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "label-studio 1.7.1 requires jsonschema==3.2.0, but you have jsonschema 4.17.3 which is incompatible."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (1.3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "label-studio 1.7.1 requires python-dateutil==2.8.1, but you have python-dateutil 2.8.2 which is incompatible."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: certifi==2022.12.7 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (2022.12.7)\n",
      "Collecting pyparsing==2.4.7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "label-studio 1.7.1 requires pytz~=2019.3, but you have pytz 2022.7 which is incompatible.\n",
      "label-studio-converter 0.0.48rc0 requires nltk==3.6.7, but you have nltk 3.7 which is incompatible.\n",
      "jupyter-server 1.13.5 requires pywinpty<2; os_name == \"nt\", but you have pywinpty 2.0.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (6.0)\n",
      "Collecting cycler==0.10.0\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (0.21.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (1.21.5)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (0.10.1)\n",
      "Requirement already satisfied: wget in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (3.2)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (9.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (2.27.1)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (1.26.14)\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (4.6.0.66)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (4.64.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from roboflow) (3.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (4.25.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\fortune\\anaconda3\\lib\\site-packages (from requests->roboflow) (2.0.4)\n",
      "Installing collected packages: pyparsing, idna, cycler\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.9\n",
      "    Uninstalling pyparsing-3.0.9:\n",
      "      Successfully uninstalled pyparsing-3.0.9\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.11.0\n",
      "    Uninstalling cycler-0.11.0:\n",
      "      Successfully uninstalled cycler-0.11.0\n",
      "Successfully installed cycler-0.10.0 idna-2.10 pyparsing-2.4.7\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in Thecowface-1 to yolov8: 100% [7540144 / 7540144] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to Thecowface-1 in yolov8:: 100%|████████████████████████| 282/282 [00:00<00:00, 825.53it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"My_api_here\")\n",
    "project = rf.workspace(\"face-cow\").project(\"thecowface\")\n",
    "dataset = project.version(1).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1de03db7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "\n            'Results' object has no attribute 'visualize'. Valid 'Results' object attributes and properties are:\n\n            Attributes:\n                boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n                masks (Masks, optional): A Masks object containing the detection masks.\n                probs (torch.Tensor, optional): A tensor containing the detection class probabilities.\n                orig_shape (tuple, optional): Original image size.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res_plotted \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize\u001b[49m()\n\u001b[0;32m      2\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m, res_plotted)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ultralytics\\yolo\\engine\\results.py:103\u001b[0m, in \u001b[0;36mResults.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[0;32m    102\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Valid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object attributes and properties are:\u001b[39m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124m        Attributes:\u001b[39m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124m            boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124m            masks (Masks, optional): A Masks object containing the detection masks.\u001b[39m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124m            probs (torch.Tensor, optional): A tensor containing the detection class probabilities.\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124m            orig_shape (tuple, optional): Original image size.\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: \n            'Results' object has no attribute 'visualize'. Valid 'Results' object attributes and properties are:\n\n            Attributes:\n                boxes (Boxes, optional): A Boxes object containing the detection bounding boxes.\n                masks (Masks, optional): A Masks object containing the detection masks.\n                probs (torch.Tensor, optional): A tensor containing the detection class probabilities.\n                orig_shape (tuple, optional): Original image size.\n            "
     ]
    }
   ],
   "source": [
    "res_plotted = results[0].visualize()\n",
    "cv2.imshow(\"result\", res_plotted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Cow Face Segmentation Above ######################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87528440",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Cow Face Identification below ######################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518c20b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ec28d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the videos to read each frame \n",
    "cap = cv2.VideoCapture(\"DATA/test/c22/IMG_3751.MOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5cd1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of frames\n",
    "num_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f82078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display image function\n",
    "def show_pic(img_cross):\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(img_cross, cmap = \"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7787724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the frames from each video\n",
    "def get_frames(num_frame):\n",
    "    for frame in range(num_frame//2):\n",
    "        ret, img = cap.read()\n",
    "        # this rotates the image before writing to file\n",
    "        rotated_img = cv2.rotate(img, cv2.ROTATE_180)        \n",
    "        if ret == False:\n",
    "            break\n",
    "        else:\n",
    "            path = r\"DATA/test/c22\"\n",
    "            name = f'/b_c{frame + 1}.jpg' \n",
    "            # write the file\n",
    "            cv2.imwrite(f'{path}{name}', rotated_img)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11768876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this call will write the images to our training folder \n",
    "get_frames(num_frame=num_frame)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddc8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682a0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de8412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rotation_range=30,\n",
    "                               width_shift_range=0.1,\n",
    "                               height_shift_range=0.1,\n",
    "                               rescale=1/255,\n",
    "                               shear_range=0.2,\n",
    "                               zoom_range=0.2,\n",
    "                               horizontal_flip=True,\n",
    "                               fill_mode=\"nearest\"                                \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18e2036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13308 images belonging to 22 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x1fbc4c97d00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_gen.flow_from_directory(\"DATA/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b6c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use this shape for all the images \n",
    "input_shape = (550, 960, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ab32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_cow = cv2.imread('DATA/train/c1/a_c102.jpg')\n",
    "the_cow = cv2.cvtColor(the_cow, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e606cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3840, 2160, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_cow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4bc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(the_cow, (550, 960), interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e50be134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 550, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c0d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pic(image_gen.random_transform(resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21b0d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3534e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# adding our convulutional nueral network layers \n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), input_shape=input_shape, activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "          \n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), input_shape=input_shape, activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "          \n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), input_shape=input_shape, activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "          \n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# the dropout layer reduces overfitting by randomly turning off nuerons during training \n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# the output is binary, either cow1 or cow.. hence Dense layer is just one\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "          \n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[\"accuracy\"]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16aba84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5774e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13308 images belonging to 22 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# here we are just pointing the iterator to the path with the training data\n",
    "train_img_gen = image_gen.flow_from_directory(\"DATA/train/\",\n",
    "                                             target_size=input_shape[:2],\n",
    "                                             batch_size=batch_size,\n",
    "                                             class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be8baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see how the data classes will be presented \n",
    "train_img_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6300a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3269 images belonging to 22 classes.\n"
     ]
    }
   ],
   "source": [
    "test_img_gen = image_gen.flow_from_directory(\"DATA/test/\",\n",
    "                                             target_size=input_shape[:2],\n",
    "                                             batch_size=batch_size,\n",
    "                                             class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca564e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 1107s 7s/step - loss: 0.2830 - accuracy: 0.9409 - val_loss: 0.1895 - val_accuracy: 0.9545\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 1249s 8s/step - loss: 0.1934 - accuracy: 0.9545 - val_loss: 0.1854 - val_accuracy: 0.9545\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 1189s 8s/step - loss: 0.1926 - accuracy: 0.9545 - val_loss: 0.1864 - val_accuracy: 0.9545\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 1121s 7s/step - loss: 0.1921 - accuracy: 0.9545 - val_loss: 0.1888 - val_accuracy: 0.9545\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 1079s 7s/step - loss: 0.1917 - accuracy: 0.9545 - val_loss: 0.1858 - val_accuracy: 0.9545\n"
     ]
    }
   ],
   "source": [
    "# training the model on our images \n",
    "results = model.fit(train_img_gen, epochs=5, steps_per_epoch=150,\n",
    "                             validation_data=test_img_gen, validation_steps=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da71bec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fbc78c6880>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saving the model \n",
    "#model.save(\"Cow_model.h5\")\n",
    "# now we evaluate the model \n",
    "plt.plot(results.history[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f00d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# the_cow_1 = cv2.imread('DATA/train/c1/a_c102.jpg')\n",
    "\n",
    "# the_cow_1 = image.load_img(the_cow_1, target_size=(960, 550))\n",
    "\n",
    "# the_cow_1 = image.img_to_array(the_cow_1)\n",
    "\n",
    "# the_cow_1 = np.expand_dims(the_cow_1, axis=0)\n",
    "# the_cow_1 = the_cow_1/255\n",
    "\n",
    "the_cow_1 = tf.keras.utils.load_img('DATA/test/fake/false3.jpg', target_size=(550, 960))\n",
    "input_arr = tf.keras.utils.img_to_array(the_cow_1)\n",
    "input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "input_arr = input_arr/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea75f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7edc1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 171ms/step\n"
     ]
    }
   ],
   "source": [
    "#prediction_prob = model.predict(the_cow_1)\n",
    "#predictions = model.predict_classes(input_arr)\n",
    "\n",
    "predicted = np.argmax(model.predict(input_arr),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b3e8f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
